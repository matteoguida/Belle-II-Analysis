{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratory of Computational Physics mod. B\n",
    "# Title????\n",
    "Authors:\n",
    "Valeria Fioroni, \n",
    "Matteo Guida,\n",
    "Philipp Zehetner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from root_pandas import read_root\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read background and signal samples from ROOT files\n",
    "#BACKGROUND\n",
    "'''CONTINUUM BACKGROUND'''\n",
    "Continuum_bkg = pd.DataFrame() #empty dataframe to contain continuum bkg samples\n",
    "#export all continuum bkg root files to a single pandas dataframe\n",
    "for file in os.listdir(\"../DataBelle2/Background/Continuum/\"):\n",
    "    continuum_temp = pd.DataFrame()\n",
    "    continuum_temp = read_root(\"../DataBelle2/Background/Continuum/\" + file, 'B0')\n",
    "    frames=[Continuum_bkg, continuum_temp]\n",
    "    Continuum_bkg= pd.concat(frames, ignore_index=True)\n",
    "    \n",
    "    \n",
    "'''PEAKING BACKGROUND'''\n",
    "Peaking_bkg = pd.DataFrame() #empty dataframe to contain peaked bkg samples\n",
    "#export all peaking bkg root files to a single pandas dataframe\n",
    "for file in os.listdir(\"../DataBelle2/Background/Peaking/\"):\n",
    "    peaking_temp = pd.DataFrame()\n",
    "    peaking_temp = read_root(\"../DataBelle2/Background/Peaking/\" + file, 'B0')\n",
    "    frames=[Peaking_bkg, peaking_temp]\n",
    "    Peaking_bkg= pd.concat(frames, ignore_index=True)\n",
    "\n",
    "#SIGNAL\n",
    "'''SIGNAL'''\n",
    "Signal = pd.DataFrame() #empty dataframe to contain signal samples\n",
    "#export all peaking bkg root files to a single pandas dataframe\n",
    "for file in os.listdir(\"../DataBelle2/Signal/\"):\n",
    "    signal_temp = pd.DataFrame()\n",
    "    signal_temp = read_root(\"../DataBelle2/Signal/\" + file, 'B0')\n",
    "    frames=[Signal, signal_temp]\n",
    "    Signal= pd.concat(frames, ignore_index=True) \n",
    "       \n",
    "\n",
    "#Signal: require B0_isSignal == True\n",
    "Signal=Signal[Signal['B0_isSignal']== True]\n",
    "#reset row indexing\n",
    "Signal.reset_index(drop=True, inplace=True)\n",
    "print(Signal.shape)\n",
    "\n",
    "#Continuum: require B0_isContinuumEvent == True\n",
    "Continuum_bkg=Continuum_bkg[Continuum_bkg['B0_isContinuumEvent']== True]\n",
    "#reset row indexing\n",
    "Continuum_bkg.reset_index(drop=True, inplace=True)\n",
    "print(Continuum_bkg.shape)\n",
    "\n",
    "#Peaking: require B0_isNotContinuumEvent == True\n",
    "Peaking_bkg=Peaking_bkg[Peaking_bkg['B0_isNotContinuumEvent']== True]\n",
    "#reset row indexing\n",
    "Peaking_bkg.reset_index(drop=True, inplace=True)\n",
    "print(Peaking_bkg.shape)\n",
    "\n",
    "def memory(df):\n",
    "    print(str((round(df.memory_usage(deep=True).sum() / 1024 ** 2, 2))),\"Mb\")\n",
    "    \n",
    "memory(Signal)\n",
    "memory(Continuum_bkg)\n",
    "memory(Peaking_bkg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful lists of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "General = ['exp_no', 'run_no', 'evt_no', 'nCands', 'iCand']\n",
    "Masses = ['B0_M', 'B0_ErrM', 'B0_SigM', 'B0_K_S0_M', 'B0_K_S0_ErrM', 'B0_K_S0_SigM', 'B0_etap_M', 'B0_etap_ErrM', \n",
    "          'B0_etap_SigM', 'B0_etap_eta_M', 'B0_etap_eta_ErrM', 'B0_etap_eta_SigM']\n",
    "Kinetics = ['B0_P', 'B0_P4', 'B0_deltae', 'B0_mbc', 'B0_etap_P', 'B0_etap_P4', \n",
    "            'B0_etap_eta_P', 'B0_etap_eta_P4', 'B0_etap_eta_gamma0_P', 'B0_etap_eta_gamma0_P4',\n",
    "            'B0_etap_eta_gamma1_P', 'B0_etap_eta_gamma1_P4', 'B0_etap_pi0_P', 'B0_etap_pi0_P4',\n",
    "            'B0_etap_pi1_P', 'B0_etap_pi1_P4', 'B0_K_S0_P', 'B0_K_S0_P4']\n",
    "Kinetics_comp = ['B0_P', 'B0_P40', 'B0_P41', 'B0_P42', 'B0_P43', 'B0_deltae', 'B0_mbc',\n",
    "                 'B0_etap_P', 'B0_etap_P40', 'B0_etap_P41', 'B0_etap_P42','B0_etap_P43','B0_etap_eta_P',\n",
    "                 'B0_etap_eta_P40', 'B0_etap_eta_P41', 'B0_etap_eta_P42', 'B0_etap_eta_P43',\n",
    "                 'B0_etap_eta_gamma0_P', 'B0_etap_eta_gamma0_P40', 'B0_etap_eta_gamma0_P41', \n",
    "                 'B0_etap_eta_gamma0_P42', 'B0_etap_eta_gamma0_P43', 'B0_etap_eta_gamma1_P', 'B0_etap_eta_gamma1_P40',\n",
    "                 'B0_etap_eta_gamma1_P41', 'B0_etap_eta_gamma1_P42', 'B0_etap_eta_gamma1_P43', 'B0_etap_pi0_P',\n",
    "                 'B0_etap_pi0_P40', 'B0_etap_pi0_P41', 'B0_etap_pi0_P42', 'B0_etap_pi0_P43', 'B0_etap_pi1_P',\n",
    "                 'B0_etap_pi1_P40', 'B0_etap_pi1_P41', 'B0_etap_pi1_P42', 'B0_etap_pi1_P43', 'B0_K_S0_P',\n",
    "                 'B0_K_S0_P40', 'B0_K_S0_P41', 'B0_K_S0_P42', 'B0_K_S0_P43']\n",
    "FourMomenta = ['B0_P4', 'B0_etap_P4', 'B0_etap_eta_P4', 'B0_etap_eta_gamma0_P4', 'B0_etap_eta_gamma1_P4', \n",
    "            'B0_etap_pi0_P4', 'B0_etap_pi1_P4', 'B0_K_S0_P4']\n",
    "DecayAngles = ['B0_decayAngle__bo0__bc', 'B0_decayAngle__bo1__bc', 'B0_etap_decayAngle__bo0__bc',\n",
    "               'B0_etap_decayAngle__bo1__bc', 'B0_etap_decayAngle__bo2__bc']\n",
    "Positions = ['B0_X', 'B0_ErrX', 'B0_Y', 'B0_ErrY', 'B0_Z', 'B0_ErrZ', 'B0_Rho',  \n",
    "             'B0_etap_X', 'B0_etap_ErrX', 'B0_etap_Y', 'B0_etap_ErrY', \n",
    "             'B0_etap_Z', 'B0_etap_ErrZ', 'B0_etap_Rho',\n",
    "             'B0_etap_eta_X', 'B0_etap_eta_ErrX', 'B0_etap_eta_Y',\n",
    "             'B0_etap_eta_ErrY', 'B0_etap_eta_Z', 'B0_etap_eta_ErrZ', 'B0_etap_eta_Rho',\n",
    "             'B0_etap_pi0_X', 'B0_etap_pi0_ErrX', 'B0_etap_pi0_Y', 'B0_etap_pi0_ErrY', \n",
    "             'B0_etap_pi0_Z', 'B0_etap_pi0_ErrZ', 'B0_etap_pi0_Rho', \n",
    "             'B0_etap_pi1_X', 'B0_etap_pi1_ErrX', 'B0_etap_pi1_Y', 'B0_etap_pi1_ErrY', \n",
    "             'B0_etap_pi1_Z', 'B0_etap_pi1_ErrZ', 'B0_etap_pi1_Rho', \n",
    "             'B0_K_S0_X', 'B0_K_S0_ErrX', 'B0_K_S0_Y', 'B0_K_S0_ErrY', 'B0_K_S0_Z',\n",
    "             'B0_K_S0_ErrZ', 'B0_K_S0_Rho', \n",
    "             'B0_cosAngleBetweenMomentumAndVertexVector', 'B0_distance', 'B0_significanceOfDistance',\n",
    "             'B0_dr', 'B0_etap_pi0_dr', 'B0_etap_pi1_dr', 'B0_K_S0_dr']\n",
    "Vertex = ['B0_VtxPvalue', 'B0_VtxProd', 'B0_VtxProdCov', 'B0_etap_VtxPvalue', 'B0_etap_VtxProd', 'B0_etap_VtxProdCov',\n",
    "          'B0_etap_eta_VtxPvalue', 'B0_etap_eta_VtxProd', 'B0_etap_VtxProdCov', \n",
    "          'B0_etap_pi0_VtxPvalue', 'B0_etap_pi0_VtxProd', 'B0_etap_pi0_VtxProdCov', \n",
    "          'B0_etap_pi1_VtxPvalue', 'B0_etap_pi1_VtxProd', 'B0_etap_pi1_VtxProdCov', \n",
    "          'B0_K_S0_VtxPvalue', 'B0_K_S0_VtxProd', 'B0_K_S0_VtxProdCov']\n",
    "\n",
    "Vertex_train = ['B0_VtxPvalue', 'B0_etap_VtxPvalue','B0_etap_eta_VtxPvalue', 'B0_etap_pi0_VtxPvalue', \n",
    "                'B0_etap_pi1_VtxPvalue', 'B0_K_S0_VtxPvalue']\n",
    "Continuum_Supression = ['B0_CSMVA', 'B0_TrCSMVA']\n",
    "\n",
    "\n",
    "Important_variables= Masses + Kinetics + DecayAngles + Positions + Vertex_train + Continuum_Supression\n",
    "print(Important_variables)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preselection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Masses from PDG\n",
    "M_B0 = [5.27955, 0.00026]\n",
    "M_K0 = [0.497611, 0.000013]\n",
    "M_etap = [0.95778, 0.00006]\n",
    "M_eta = [0.547862, 0.000017]\n",
    "\n",
    "#Require the B0 mass to be within 5 sigma from PDG value\n",
    "Signal = Signal[abs(Signal['B0_M'] - M_B0[0]) < 5 * Signal['B0_ErrM']]\n",
    "print(Signal.shape)\n",
    "\n",
    "#In case of more than 1 candidate per event select the first one occuring\n",
    "Mask_duplicated = Signal.duplicated(subset='evt_no', keep='first')\n",
    "Mask_duplicated=np.logical_not(Mask_duplicated)\n",
    "Signal=Signal[Mask_duplicated]\n",
    "Signal.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(Signal.shape)\n",
    "print(Signal['evt_no'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select variables useful for the Classification task\n",
    "Signal_sel = Signal[Important_variables]\n",
    "del(Signal)\n",
    "Continuum_bkg_sel= Continuum_bkg[Important_variables]\n",
    "del(Continuum_bkg)\n",
    "Peaking_bkg_sel= Peaking_bkg[Important_variables]\n",
    "del(Peaking_bkg)\n",
    "\n",
    "print(Continuum_bkg_sel.shape)\n",
    "print(Peaking_bkg_sel.shape)\n",
    "print(Signal_sel.shape)\n",
    "\n",
    "#The Dataframe contains arrays as elements, I create a new column for each component\n",
    "for M in FourMomenta: \n",
    "    Signal_sel[M+'0'], Signal_sel[M+'1'], Signal_sel[M+'2'], Signal_sel[M+'3'] = zip(*Signal_sel.pop(M))\n",
    "    Continuum_bkg_sel[M+'0'], Continuum_bkg_sel[M+'1'], Continuum_bkg_sel[M+'2'], Continuum_bkg_sel[M+'3'] = zip(*Continuum_bkg_sel.pop(M))\n",
    "    Peaking_bkg_sel[M+'0'], Peaking_bkg_sel[M+'1'], Peaking_bkg_sel[M+'2'], Peaking_bkg_sel[M+'3'] = zip(*Peaking_bkg_sel.pop(M))\n",
    "\n",
    "#Signal_sel[['B0_P40', 'B0_P41', 'B0_P42', 'B0_P43']]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "pd.options.display.max_columns = None\n",
    "display(Signal_sel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "I tried a feature scaling on the dataset.\n",
    "Centered the points around their mean and scaled by their sample standard deviation. \n",
    "It seems that in general the algorithms of machine learning algorithms perform better after this kind of scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import seed\n",
    "from random import randint\n",
    "# seed random number generator\n",
    "n_seed=2347\n",
    "seed(n_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import train_test_split\n",
    "Signal_sel['Type'] = 2 #Signal label == 2\n",
    "Peaking_bkg_sel['Type'] = 1 # Peaking background label == 1\n",
    "Continuum_bkg_sel['Type'] = 0 #Continuum backgroun label == 0\n",
    "\n",
    "# Put in a dataframe randomly shuffled the lines\n",
    "Sum_BS = pd.concat([Signal_sel,Peaking_bkg_sel,Continuum_bkg_sel]).sample(frac=1)\n",
    "\n",
    "del(Signal_sel)\n",
    "del(Peaking_bkg_sel)\n",
    "del(Continuum_bkg_sel)\n",
    "\n",
    "X = Sum_BS.drop('Type',axis=1)\n",
    "Y = Sum_BS['Type']\n",
    "\n",
    "del(Sum_BS)\n",
    "\n",
    "# Binarize the labels\n",
    "Y = label_binarize(Y, classes=[0, 1, 2])\n",
    "n_classes = Y.shape[1]\n",
    "print(Y)\n",
    "\n",
    "# Dataframe divided into 80% of train data and 20% of test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.8,random_state=randint(10**6,10**9))\n",
    "# Standardize features by removing the mean and scaling to unit variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization of a dataset is a common requirement for many machine learning estimators: \n",
    "they might behave badly if the individual features do not more or less look like standard normally \n",
    "distributed data (e.g. Gaussian with 0 mean and unit variance). [More infos](https://scikit-learn.org/stable/modules/preprocessing.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "print(ss.fit(X_train))\n",
    "# The averages and standard deviations of all the databases are saved, \n",
    "# now with transfer I modify the data (Train and Test) in practice\n",
    "X_train = ss.transform(X_train)\n",
    "X_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Optimization\n",
    "\n",
    "# ************** DRAFT **************\n",
    "in \"parameters\" I would like to put also different possibilities for learning rate and solver at least. Maybe just 3 different learning rates and 2 different solvers, otherwise it could take too long to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'hidden_layer_sizes': [(100,300,200,100,), (100,200, 400,200,100,), (200, 300, 400, 500, 300,)],\n",
    "              'solver': ['sgd', 'adam'],\n",
    "              #'learning_rate_init': [0.1, 0.01, 0.001]\n",
    "              'learning_rate_init': [0.1, 0.01]\n",
    "             }\n",
    "\n",
    "\n",
    "mlp = MLPClassifier(max_iter=300, alpha=1e-4,\n",
    "                    tol=1e-4, random_state=n_seed)\n",
    "\n",
    "clf= GridSearchCV(mlp,param_grid=parameters,verbose=2,cv=5,return_train_score=True)\n",
    "\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "print ('RESULTS FOR NN\\n')\n",
    "print(\"Best parameters set found:\")\n",
    "print(clf.best_params_)\n",
    "\n",
    "print(\"Score with best parameters:\")\n",
    "print(clf.best_score_)\n",
    "\n",
    "print(\"\\nAll scores on the grid:\")\n",
    "scores=pd.DataFrame(clf.cv_results_)\n",
    "print(scores[['param_hidden_layer_sizes', 'param_solver', 'param_learning_rate_init', 'mean_test_score', 'mean_train_score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get training and test error for the best NN model from CV\n",
    "\n",
    "best_mlp = MLPClassifier(hidden_layer_sizes=clf.best_params_['hidden_layer_sizes'], max_iter=300, alpha=1e-4,\n",
    "                    solver=clf.best_params_['solver'], tol=1e-4, random_state=n_seed,\n",
    "                    learning_rate_init=clf.best_params_['learning_rate_init'])\n",
    "\n",
    "best_mlp.fit(X_train, Y_train)\n",
    "\n",
    "print(best_mlp.predict_proba(X_train))\n",
    "\n",
    "training_error = 1. - best_mlp.score(X_train,Y_train)\n",
    "test_error = 1. - best_mlp.score(X_test,Y_test)\n",
    "\n",
    "print ('\\nRESULTS FOR BEST NN\\n')\n",
    "\n",
    "print (\"Best NN training error: %f\" % training_error)\n",
    "print (\"Best NN test error: %f\" % test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code from https://scikit-learn.org/0.15/auto_examples/plot_roc.html\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp\n",
    "import matplotlib.pyplot as plt\n",
    "#from itertools import cycle\n",
    "\n",
    "y_score = best_mlp.predict_proba(X_test)\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(Y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(Y_test.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "print(Y_test.ravel())\n",
    "# Plot of a ROC curve for a specific class\n",
    "plt.figure()\n",
    "plt.plot(fpr[2], tpr[2], label='ROC curve (area = %0.2f)' % roc_auc[2])\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]))\n",
    "for i in range(n_classes):\n",
    "    plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                                   ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
